{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "255f1b40-962f-4959-8b95-f5e1540e27e7",
   "metadata": {},
   "source": [
    "## **Short Term Memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db475c12-3dcb-461f-a9f7-80a0206fc244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic': 'pizza', 'joke': 'Why did the pizza go to the doctor? Because it had a big lumpy tumor!', 'explanation': 'The punchline of this joke is that the pizza was so large and full of cheese that it caused a lump in its center, which is why it needed medical attention. The joke plays on the idea that something (in this case, the pizza) can be too much or too big for someone to handle, leading them to seek out medical help.\\nThe punchline also touches on the concept of \"big lumps\" and how they can cause discomfort or pain. In this case, it\\'s a metaphorical way of saying that something (in this case, the pizza) is so large and full of cheese that it caused a lump in its center, which is why it needed medical attention.\\nOverall, the punchline of this joke is that the pizza was too big for someone to handle, leading them to seek out medical help.'}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.checkpoint.memory import InMemorySaver  # Save in RAM\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"qwen2.5:0.5b\",\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "class JokeState(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    explanation: str\n",
    "\n",
    "\n",
    "# Node\n",
    "def generate_joke(state: JokeState):\n",
    "    prompt = f\"Generate a joke on the topic: {state['topic']}\"\n",
    "    response = model.invoke(prompt).content\n",
    "    return {'joke': response}\n",
    "\n",
    "def generate_explanation(state: JokeState):\n",
    "    prompt = f\"Write an explanation for the joke: {state['joke']}\"\n",
    "    response = model.invoke(prompt).content\n",
    "    return {'explanation': response}\n",
    "\n",
    "\n",
    "# Graph\n",
    "graph = StateGraph(JokeState)\n",
    "\n",
    "# Node\n",
    "graph.add_node('generate_joke', generate_joke)\n",
    "graph.add_node('generate_explanation', generate_explanation)\n",
    "\n",
    "# Edge\n",
    "graph.add_edge(START, 'generate_joke')\n",
    "graph.add_edge('generate_joke', 'generate_explanation')\n",
    "graph.add_edge('generate_explanation', END)\n",
    "\n",
    "# Persistence\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "# Compile\n",
    "workflow = graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "config1 = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "response = workflow.invoke({'topic': 'pizza'}, config=config1)\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7268c5-9e04-4936-b23c-310e5fb5db89",
   "metadata": {},
   "source": [
    "### **SQLiteServer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcf773b8-cfc6-43f2-928e-c558a596e976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic': 'pizza', 'joke': 'Why did the pizza go to the doctor?\\n\\nBecause it had an ulcer!', 'explanation': 'The joke \"Why did the pizza go to the doctor?\" is a play on words and a classic example of a pun. Let\\'s break down the punchline:\\n\\n1. **\"Why did the pizza go to the doctor? Because it had an ulcer!\"**\\n   - The punchline uses the word \"ulcer\" as a double entendre, which means both a physical injury (like a cut or tear) and a medical condition.\\n   - In this case, the punchline is about the pizza\\'s physical damage rather than its health status.\\n\\n2. **The punchline:**\\n   - \"Because it had an ulcer!\" \\n     - Here, \"ulcer\" refers to the physical injury that caused the problem with the pizza.\\n     - The punchline suggests that the pizza was injured or damaged in some way by something external (the doctor).\\n\\n3. **Why did the pizza go to the doctor?**\\n   - The joke is about the pizza\\'s health status rather than its physical condition.\\n   - It implies that the pizza had a problem with it, which could be related to its ingredients, packaging, or even the person who made it.\\n\\nIn summary, the punchline \"Because it had an ulcer!\" plays on the word \"ulcer\" by using it as both a medical term and a physical injury. The joke is about how the pizza\\'s health status (injury) led it to seek medical attention rather than its actual condition or physical damage.'}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "# -------------------- MODEL --------------------\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"qwen2.5:0.5b\",\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "# -------------------- STATE --------------------\n",
    "\n",
    "class JokeState(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    explanation: str\n",
    "\n",
    "# -------------------- NODES --------------------\n",
    "\n",
    "def generate_joke(state: JokeState):\n",
    "    prompt = f\"Generate a joke on the topic: {state['topic']}\"\n",
    "    response = model.invoke(prompt).content\n",
    "    return {\"joke\": response}\n",
    "\n",
    "def generate_explanation(state: JokeState):\n",
    "    prompt = f\"Write an explanation for the joke: {state['joke']}\"\n",
    "    response = model.invoke(prompt).content\n",
    "    return {\"explanation\": response}\n",
    "\n",
    "# -------------------- GRAPH --------------------\n",
    "\n",
    "graph = StateGraph(JokeState)\n",
    "\n",
    "graph.add_node(\"generate_joke\", generate_joke)\n",
    "graph.add_node(\"generate_explanation\", generate_explanation)\n",
    "\n",
    "graph.add_edge(START, \"generate_joke\")\n",
    "graph.add_edge(\"generate_joke\", \"generate_explanation\")\n",
    "graph.add_edge(\"generate_explanation\", END)\n",
    "\n",
    "# -------------------- RUN WITH SQLITE MEMORY --------------------\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"joke-thread-1\"  # Same ID = same memory\n",
    "    }\n",
    "}\n",
    "\n",
    "# IMPORTANT: Open SqliteSaver correctly\n",
    "with SqliteSaver.from_conn_string(\"joke_memory.sqlite\") as checkpointer:\n",
    "    workflow = graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "    response = workflow.invoke(\n",
    "        {\"topic\": \"pizza\"},\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9382618e-5243-4e9b-af53-5a4b216e985a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Joke Generation Workflow ===\n",
      "\n",
      "Generating joke about: pizza\n",
      "Generating explanation for the joke...\n",
      "\n",
      "Workflow completed!\n",
      "\n",
      "Topic: pizza\n",
      "Joke: Why did the pizza go to therapy? Because it had too many tummy problems!\n",
      "Explanation: The punchline of \"Why did the pizza go to therapy?\" is that the joke is playing on a common misunderstanding about pizza and its digestive system. The punchline suggests that the pizza went to therapy because it has too many tummy problems, which is an absurdly exaggerated way of saying that the pizza had too many stomach issues or was causing discomfort.\n",
      "The punchline is also clever in that it's not just a simple repetition of the original statement but rather a play on words. The use of \"too many tummy problems\" adds a layer of humor by making the punchline more absurd than the original statement, which makes it even more memorable and funny to hear.\n",
      "\n",
      "==================================================\n",
      "CURRENT CHECKPOINTED STATE\n",
      "Values: {'topic': 'pizza', 'joke': 'Why did the pizza go to therapy? Because it had too many tummy problems!', 'explanation': 'The punchline of \"Why did the pizza go to therapy?\" is that the joke is playing on a common misunderstanding about pizza and its digestive system. The punchline suggests that the pizza went to therapy because it has too many tummy problems, which is an absurdly exaggerated way of saying that the pizza had too many stomach issues or was causing discomfort.\\nThe punchline is also clever in that it\\'s not just a simple repetition of the original statement but rather a play on words. The use of \"too many tummy problems\" adds a layer of humor by making the punchline more absurd than the original statement, which makes it even more memorable and funny to hear.'}\n",
      "Next node(s): None (completed)\n",
      "\n",
      "CHECKPOINT HISTORY (most recent first):\n",
      "  1. Step 10 → next: END\n",
      "  2. Step 9 → next: ('generate_explanation',)\n",
      "  3. Step 8 → next: ('generate_joke',)\n",
      "  4. Step 7 → next: ('__start__',)\n",
      "  5. Step 6 → next: END\n",
      "  6. Step 5 → next: ('generate_explanation',)\n",
      "  7. Step 4 → next: ('generate_joke',)\n",
      "  8. Step 3 → next: ('__start__',)\n",
      "  9. Step 2 → next: END\n",
      "  10. Step 1 → next: ('generate_explanation',)\n",
      "  11. Step 0 → next: ('generate_joke',)\n",
      "  12. Step -1 → next: ('__start__',)\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from typing import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "# -------------------- MODEL --------------------\n",
    "model = ChatOllama(\n",
    "    model=\"qwen2.5:0.5b\",\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "# -------------------- STATE --------------------\n",
    "class JokeState(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    explanation: str\n",
    "\n",
    "# -------------------- NODES --------------------\n",
    "def generate_joke(state: JokeState):\n",
    "    print(f\"Generating joke about: {state['topic']}\")\n",
    "    prompt = f\"Generate a short, funny joke about {state['topic']}. Respond with ONLY the joke, no extra text.\"\n",
    "    response = model.invoke(prompt).content.strip()\n",
    "    return {\"joke\": response}\n",
    "\n",
    "def generate_explanation(state: JokeState):\n",
    "    print(\"Generating explanation for the joke...\")\n",
    "    prompt = f\"Explain why this joke is funny in 2-3 sentences:\\n\\n{state['joke']}\"\n",
    "    response = model.invoke(prompt).content.strip()\n",
    "    return {\"explanation\": response}\n",
    "\n",
    "# -------------------- GRAPH --------------------\n",
    "builder = StateGraph(JokeState)\n",
    "\n",
    "builder.add_node(\"generate_joke\", generate_joke)\n",
    "builder.add_node(\"generate_explanation\", generate_explanation)\n",
    "\n",
    "builder.add_edge(START, \"generate_joke\")\n",
    "builder.add_edge(\"generate_joke\", \"generate_explanation\")\n",
    "builder.add_edge(\"generate_explanation\", END)\n",
    "\n",
    "# -------------------- PERSISTENT CHECKPOINTER (MANUAL CONNECTION) --------------------\n",
    "# Persistent connection - keeps database open for multiple runs and state inspection\n",
    "conn = sqlite3.connect(\"joke_memory.sqlite\", check_same_thread=False)\n",
    "checkpointer = SqliteSaver(conn)\n",
    "\n",
    "# Compile once - reusable across multiple invocations\n",
    "workflow = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"joke-thread-1\"  # Same thread_id = shared memory across runs\n",
    "    }\n",
    "}\n",
    "\n",
    "# -------------------- RUN THE WORKFLOW --------------------\n",
    "print(\"=== Starting Joke Generation Workflow ===\\n\")\n",
    "\n",
    "# On first run: provide the topic\n",
    "# On subsequent runs (if interrupted or testing): you can pass {} to resume or new input to start fresh\n",
    "input_data = {\"topic\": \"pizza\"}\n",
    "\n",
    "try:\n",
    "    result = workflow.invoke(input_data, config=config)\n",
    "    print(\"\\nWorkflow completed!\\n\")\n",
    "    print(\"Topic:\", result[\"topic\"])\n",
    "    print(\"Joke:\", result[\"joke\"])\n",
    "    print(\"Explanation:\", result[\"explanation\"])\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\nWorkflow interrupted — checkpoint automatically saved!\")\n",
    "    print(\"Re-run this script to resume from the last completed step.\")\n",
    "\n",
    "# -------------------- INSPECT STATE ANYTIME --------------------\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CURRENT CHECKPOINTED STATE\")\n",
    "current_state = workflow.get_state(config)\n",
    "print(\"Values:\", current_state.values)\n",
    "print(\"Next node(s):\", current_state.next or \"None (completed)\")\n",
    "\n",
    "# Optional: View full history\n",
    "print(\"\\nCHECKPOINT HISTORY (most recent first):\")\n",
    "for i, snapshot in enumerate(workflow.get_state_history(config)):\n",
    "    step = snapshot.metadata.get(\"step\", \"unknown\")\n",
    "    next_node = snapshot.next or \"END\"\n",
    "    print(f\"  {i+1}. Step {step} → next: {next_node}\")\n",
    "\n",
    "# Note: Keep connection open for interactive use\n",
    "# Only close when completely done (e.g., end of session):\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8666d2c2-b3c0-46cc-ab8c-bfff7ddfac35",
   "metadata": {},
   "source": [
    "### **PostgreSQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95743390-c9ab-48b4-bb3d-71342071908d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic': 'pizza', 'joke': 'Why did the pizza go to the doctor?\\n\\nBecause it had a hole in its head!', 'explanation': 'The punchline of this joke is that the pizza went to the doctor because it had a hole in its head. This is a play on words and pun, as \"hole\" can refer both to an actual physical hole or a metaphorical hole in one\\'s character or personality.\\nIn this case, the punchline suggests that the pizza was not just a regular pizza but something more serious or problematic, which led it to be seen by the doctor. The joke is meant to make a point about how people can sometimes take things too seriously and see problems as bigger than they are.\\nOverall, the punchline of this joke serves to play on the word \"hole\" in a clever way, making it an amusing and unexpected twist that leads to a humorous conclusion.'}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "\n",
    "# -------------------- MODEL --------------------\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"qwen2.5:0.5b\",\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "# -------------------- STATE --------------------\n",
    "\n",
    "class JokeState(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    explanation: str\n",
    "\n",
    "# -------------------- NODES --------------------\n",
    "\n",
    "def generate_joke(state: JokeState):\n",
    "    prompt = f\"Generate a joke on the topic: {state['topic']}\"\n",
    "    response = model.invoke(prompt).content\n",
    "    return {\"joke\": response}\n",
    "\n",
    "def generate_explanation(state: JokeState):\n",
    "    prompt = f\"Write an explanation for the joke: {state['joke']}\"\n",
    "    response = model.invoke(prompt).content\n",
    "    return {\"explanation\": response}\n",
    "\n",
    "# -------------------- GRAPH --------------------\n",
    "\n",
    "graph = StateGraph(JokeState)\n",
    "\n",
    "graph.add_node(\"generate_joke\", generate_joke)\n",
    "graph.add_node(\"generate_explanation\", generate_explanation)\n",
    "\n",
    "graph.add_edge(START, \"generate_joke\")\n",
    "graph.add_edge(\"generate_joke\", \"generate_explanation\")\n",
    "graph.add_edge(\"generate_explanation\", END)\n",
    "\n",
    "# -------------------- RUN WITH POSTGRES MEMORY --------------------\n",
    "\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"user-1-joke-thread\"  # Same ID = resume memory\n",
    "    }\n",
    "}\n",
    "\n",
    "# IMPORTANT: Use PostgresSaver as context manager\n",
    "with PostgresSaver.from_conn_string(\n",
    "    \"postgresql://postgres:abcd%401234@localhost:5432/langgraph_memory\"\n",
    ") as checkpointer:\n",
    "\n",
    "    # Create required tables (safe to call multiple times)\n",
    "    checkpointer.setup()\n",
    "\n",
    "    workflow = graph.compile(checkpointer=checkpointer)\n",
    "\n",
    "    result = workflow.invoke(\n",
    "        {\"topic\": \"pizza\"},\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3abf9a6-c969-49ca-9d22-c03a3ae1f976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response: The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "import psycopg\n",
    "from psycopg.rows import dict_row\n",
    "\n",
    "# -------------------- STATE --------------------\n",
    "\n",
    "class ChatState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "\n",
    "# -------------------- MODEL --------------------\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"qwen2.5:0.5b\",\n",
    "    temperature=0.4,\n",
    ")\n",
    "\n",
    "# -------------------- NODE --------------------\n",
    "\n",
    "def chat_node(state: ChatState) -> dict:\n",
    "    \"\"\"Node that processes chat messages - uses invoke instead of stream for compatibility\"\"\"\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# -------------------- GRAPH --------------------\n",
    "\n",
    "workflow = StateGraph(state_schema=ChatState)\n",
    "workflow.add_node(\"chat\", chat_node)\n",
    "workflow.add_edge(START, \"chat\")\n",
    "workflow.add_edge(\"chat\", END)\n",
    "\n",
    "# -------------------- CHECKPOINT --------------------\n",
    "DB_URI = \"postgresql://postgres:abcd%401234@localhost:5432/langgraph_memory\"\n",
    "\n",
    "# Create connection with proper settings for PostgresSaver\n",
    "conn = psycopg.connect(DB_URI, autocommit=True, row_factory=dict_row)\n",
    "\n",
    "# Initialize checkpointer\n",
    "checkpointer = PostgresSaver(conn)\n",
    "checkpointer.setup()  # Create tables if they don't exist\n",
    "\n",
    "# Compile chatbot with checkpointer\n",
    "chatbot = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "# -------------------- RUN --------------------\n",
    "initial_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"What is the capital of India?\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"thread-1\"}}\n",
    "\n",
    "print(\"AI Response: \", end='')\n",
    "\n",
    "# Stream the response - you can use this same pattern in other scripts\n",
    "for event in chatbot.stream(input=initial_state, config=config, stream_mode=\"values\"):\n",
    "    if \"messages\" in event and len(event[\"messages\"]) > 0:\n",
    "        last_message = event[\"messages\"][-1]\n",
    "        # Only print AI messages, not human messages\n",
    "        if hasattr(last_message, 'content') and last_message.__class__.__name__ == 'AIMessage':\n",
    "            print(last_message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5082ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
